{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dengyiliu/miniconda3/envs/llama3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2024.11.10: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A4000. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Define a LLaMA 3.2 compatible template\n",
    "template_tokenizer = get_chat_template(\n",
    "    tokenizer=tokenizer,  # Replace with your tokenizer if already defined\n",
    "    chat_template=\"llama\",  # Specify LLaMA template\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"user\", \"assistant\": \"assistant\"},\n",
    "    map_eos_token=True,  # Map <|end|> to </s> token\n",
    ")\n",
    "\n",
    "# Function to format the dataset\n",
    "def format_to_llama32(examples):\n",
    "    formatted_texts = []\n",
    "    for instruction, context, response in zip(examples[\"instruction\"], examples[\"context\"], examples[\"response\"]):\n",
    "        user_text = f\"<|user|> Instruction: {instruction}\\nContext: {context}\"\n",
    "        assistant_text = f\"<|assistant|> {response}\"\n",
    "        formatted_texts.append(f\"{user_text}\\n{assistant_text}\")\n",
    "    return {\"text\": formatted_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.10 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Australia is Canberra.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Instruction: What is the capital of Australia?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0] # Get the first (and likely only) string from the list\n",
    "response = decoded_output.split('assistant')[1].strip() if 'assistant' in decoded_output else decoded_output.strip()\n",
    "print(response)  # Print the response to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Function to generate response using the fine-tuned model\n",
    "def generate_response(model, tokenizer, input_text):\n",
    "    # Apply chat template using get_chat_template function\n",
    "    tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
    "    # Prepare messages in the chat format\n",
    "    messages = [{\"role\": \"user\", \"content\": input} for input in input_text]\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Add prompt for generation\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    print(inputs.size())\n",
    "    # Generate model output\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=128,  # Adjust as necessary\n",
    "        use_cache=True,\n",
    "        temperature=1.5,\n",
    "        min_p=0.1\n",
    "    )\n",
    "    # Decode model output\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "    return decoded_output\n",
    "\n",
    "# Generate predictions and store them in a JSON file\n",
    "def generate_predictions_and_store(dataset, model, tokenizer, output_file=\"predictions.json\"):\n",
    "    predictions = []\n",
    "    input_text = []\n",
    "    for sample in tqdm(dataset, desc=\"Generating predictions\"): # Added tqdm here\n",
    "        instruction = sample[\"instruction\"]\n",
    "        text = sample[\"text\"]\n",
    "        context = sample[\"context\"]\n",
    "        input = f\"Instruction: {instruction}\\nContext: {context}\"\n",
    "        # Combine the instruction and context with the text field for evaluation\n",
    "        input_text.append(input)\n",
    "    # Generate prediction using the model\n",
    "    predicted = generate_response(model, tokenizer, input_text)\n",
    "    print(predicted)\n",
    "    predicted = predicted.split('<|assistant|>')\n",
    "    for i, sample in enumerate(dataset):\n",
    "        instruction = sample[\"instruction\"]\n",
    "        context = sample[\"context\"]\n",
    "        text = sample[\"text\"]\n",
    "        response = sample[\"response\"]\n",
    "\n",
    "        # predicted_response = predicted[i+1].split('<|eot_id|>')[0]\n",
    "        # Store results in predictions\n",
    "        predictions.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"context\": context,\n",
    "            \"text\": text,\n",
    "            \"predicted_response\": response,\n",
    "            \"response\": response\n",
    "        })\n",
    "\n",
    "    # Write predictions to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(predictions, f, indent=4)\n",
    "    print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# def format_to_llama32_test(examples):\n",
    "#     formatted_texts = []\n",
    "#     for instruction, context, response in zip(examples[\"instruction\"], examples[\"context\"], examples[\"response\"]):\n",
    "#         user_text = f\"<|user|> Instruction: {instruction}\\nContext: {context}\"\n",
    "#         assistant_text = f\"<|assistant|> {response}\"\n",
    "#         formatted_texts.append(f\"{user_text}\")\n",
    "#     return {\"text\": formatted_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Function to generate response using the fine-tuned model\n",
    "def generate_response(model, tokenizer, input_text):\n",
    "    # Apply chat template using get_chat_template function\n",
    "    tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")\n",
    "    # Prepare messages in the chat format\n",
    "    messages = [{\"role\": \"user\", \"content\": input_text},]\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,  # Add prompt for generation\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate model output\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=128,  # Adjust as necessary\n",
    "        use_cache=True,\n",
    "        temperature=1.5,\n",
    "        min_p=0.1\n",
    "    )\n",
    "    # Decode model output\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "    return decoded_output\n",
    "\n",
    "# Generate predictions and store them in a JSON file\n",
    "def generate_predictions_and_store(dataset, model, tokenizer, output_file=\"predictions.json\"):\n",
    "    predictions = []\n",
    "    input_text = []\n",
    "    for sample in tqdm(dataset, desc=\"Generating predictions\"): # Added tqdm here\n",
    "        instruction = sample[\"instruction\"]\n",
    "        text = sample[\"text\"]\n",
    "        context = sample[\"context\"]\n",
    "        input = f\"Instruction: {instruction}\\nContext: {context}\"\n",
    "        response = sample[\"response\"]\n",
    "\n",
    "        # Generate prediction using the model\n",
    "        predicted = generate_response(model, tokenizer, input)\n",
    "        predicted = predicted.split('<|start_header_id|>assistant<|end_header_id|>')[1]\n",
    "        predictions.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"context\": context,\n",
    "            \"text\": text,\n",
    "            \"predicted_response\": predicted,\n",
    "            \"response\": response\n",
    "        })\n",
    "\n",
    "    # Write predictions to JSON\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(predictions, f, indent=4)\n",
    "    print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 353/353 [00:00<00:00, 80357.63 examples/s]\n",
      "Generating predictions:   0%|          | 0/353 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 353/353 [58:40<00:00,  9.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /mnt/c/Users/IoT_lab_YU/dengyiliu/llama3/llama3.2finetune/llama3.2/evaluate/predictions_8b_beforeft_test_dataset_brainstorming.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 427/427 [00:00<00:00, 35613.51 examples/s]\n",
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 427/427 [50:16<00:00,  7.06s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /mnt/c/Users/IoT_lab_YU/dengyiliu/llama3/llama3.2finetune/llama3.2/evaluate/predictions_8b_beforeft_test_dataset_classification.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 355/355 [00:00<00:00, 37218.87 examples/s]\n",
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 355/355 [32:44<00:00,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /mnt/c/Users/IoT_lab_YU/dengyiliu/llama3/llama3.2finetune/llama3.2/evaluate/predictions_8b_beforeft_test_dataset_closed_qa.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [00:00<00:00, 16048.91 examples/s]\n",
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [23:54<00:00, 10.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /mnt/c/Users/IoT_lab_YU/dengyiliu/llama3/llama3.2finetune/llama3.2/evaluate/predictions_8b_beforeft_test_dataset_creative_writing.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [00:00<00:00, 18893.46 examples/s]\n",
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438/438 [1:02:36<00:00,  8.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /mnt/c/Users/IoT_lab_YU/dengyiliu/llama3/llama3.2finetune/llama3.2/evaluate/predictions_8b_beforeft_test_dataset_general_qa.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 301/301 [00:00<00:00, 6375.64 examples/s]\n",
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 301/301 [24:51<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /mnt/c/Users/IoT_lab_YU/dengyiliu/llama3/llama3.2finetune/llama3.2/evaluate/predictions_8b_beforeft_test_dataset_information_extraction.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 749/749 [00:00<00:00, 28359.08 examples/s]\n",
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 749/749 [1:38:56<00:00,  7.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /mnt/c/Users/IoT_lab_YU/dengyiliu/llama3/llama3.2finetune/llama3.2/evaluate/predictions_8b_beforeft_test_dataset_open_qa.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 238/238 [00:00<00:00, 11781.06 examples/s]\n",
      "Generating predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 238/238 [32:41<00:00,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to /mnt/c/Users/IoT_lab_YU/dengyiliu/llama3/llama3.2finetune/llama3.2/evaluate/predictions_8b_beforeft_test_dataset_summarization.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_data_dir = \"/mnt/c/Users/IoT_lab_YU/dengyiliu/llama3/llama3.2finetune/llama3.2/data/test_categories\"  # Replace with your actual directory\n",
    "\n",
    "for filename in os.listdir(test_data_dir):\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        file_path = os.path.join(test_data_dir, filename)\n",
    "        try:\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "            dataset_test = Dataset.from_pandas(df)\n",
    "            dataset_test = dataset_test.map(format_to_llama32, batched=True)\n",
    "            output_filename = f\"/mnt/c/Users/IoT_lab_YU/dengyiliu/llama3/llama3.2finetune/llama3.2/evaluate/predictions_8b_beforeft_{filename.split('.')[0]}.jsonl\" #create output filename\n",
    "            generate_predictions_and_store(dataset_test, model, tokenizer, output_file=output_filename)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
